#!/usr/bin/env node
/**
 * ADR Prompt Tuning Tool
 *
 * Provides automated testing and refinement of ADR generation prompts.
 * Tests prompts against realistic ADR scenarios and scores output quality.
 *
 * Usage:
 *   node tools/prompt-tuner.js test <phase> [--mock]
 *   node tools/prompt-tuner.js analyze-results
 *   node tools/prompt-tuner.js suggest-improvements <phase>
 */

import fs from "fs";
import path from "path";
import ADRScorer from "./adr-scorer.js";

class ADRPromptTuner {
  constructor() {
    this.scorer = new ADRScorer();
    this.resultsDir = "./prompt_tuning_results_architecture-decision-record";
    this.prompDir = "./prompts";
    this.testCases = this.loadTestCases();
  }

  loadTestCases() {
    return [
      {
        id: "test_001",
        name: "Monolithic to Microservices Migration",
        status: "Proposed",
        context: `Our e-commerce platform is built as a monolith with growing scalability issues. 
        Customer base has grown 300% in 18 months, causing database bottlenecks and slow deployments.
        The system has tight coupling between order processing, inventory management, and payment processing.
        Deployments take 45 minutes and lock the database, causing 2-3 hour outages weekly.
        Our team now spans 5 domains with conflicting deployment schedules.`,
        expectedDecisionKeywords: ["microservices", "domain boundaries", "independently deploy"]
      },
      {
        id: "test_002",
        name: "Frontend Framework Migration",
        status: "Proposed",
        context: `Our 5-year-old jQuery-based admin dashboard has become unmaintainable.
        It takes new developers 6 weeks to become productive. The codebase has no tests.
        We're losing features to competitors with better UX. Customer feedback shows frustration with performance.
        Our team of 8 frontend developers spends 40% of time on bug fixes rather than features.
        Modern frameworks offer better component reusability and testing support.`,
        expectedDecisionKeywords: ["framework", "components", "testing", "maintainability"]
      },
      {
        id: "test_003",
        name: "Database Technology Selection",
        status: "Proposed",
        context: `Our relational database stores 500GB of primarily unstructured operational logs and events.
        Queries for analytics take 30+ minutes. Compliance requires we retain data for 7 years.
        We're running out of disk space despite aggressive archiving.
        Data structure is mostly key-value pairs with variable attributes.
        Current quarterly costs are $150k for database infrastructure alone.`,
        expectedDecisionKeywords: ["time-series", "columnar", "compress", "cost", "retention"]
      },
      {
        id: "test_004",
        name: "Authentication Strategy",
        status: "Proposed",
        context: `Currently, each service manages its own user authentication using session cookies.
        This causes inconsistent security policies and hard-to-audit login attempts.
        We have 8 different services with 8 different password policies.
        Security audit flagged 12 vulnerabilities related to inconsistent auth implementation.
        New compliance requirement mandates multi-factor authentication across all services.`,
        expectedDecisionKeywords: ["centralized", "OAuth", "SSO", "consistent", "audit"]
      },
      {
        id: "test_005",
        name: "API Versioning Strategy",
        status: "Proposed",
        context: `Our public API has 1,200 active integrations with third-party developers.
        We need to make breaking changes to support new business requirements.
        We've deprecated v1 twice already, each time causing partner outages and support burden.
        Current approach of maintaining multiple endpoint versions causes code duplication and inconsistency.
        Partners request longer deprecation timelines (currently 6 months, asking for 18 months).`,
        expectedDecisionKeywords: ["versioning", "semantic", "backward-compatible", "deprecation"]
      }
    ];
  }

  /**
   * Test a specific phase's prompt
   */
  async testPhase(phase) {
    console.log(`\nüß™ Testing Phase ${phase.toUpperCase()} Prompts`);
    console.log("=" .repeat(60));

    const promptPath = path.join(this.prompDir, `${phase}.md`);
    if (!fs.existsSync(promptPath)) {
      console.error(`‚ùå Prompt file not found: ${promptPath}`);
      return null;
    }

    const results = [];

    for (const testCase of this.testCases) {
      console.log(`\nüìù Test Case: ${testCase.name} (${testCase.id})`);
      
      // In real implementation, this would call actual LLM
      // For now, we'll generate a mock response
      const output = this.generateMockOutput(testCase, phase);
      const score = this.scorer.score(output);

      results.push({
        testCaseId: testCase.id,
        testCaseName: testCase.name,
        phase: phase,
        output: output,
        score: score.overall,
        details: score.details
      });

      console.log(`  Score: ${score.overall.toFixed(2)}/5.0`);
      console.log(`  Completeness: ${score.completeness.toFixed(2)}/5.0`);
      console.log(`  Clarity: ${score.clarity.toFixed(2)}/5.0`);
      console.log(`  Consequences Balance: ${score.consequencesBalance.toFixed(2)}/5.0`);
    }

    // Save results
    this.saveResults(phase, results);
    
    // Calculate average score
    const avgScore = results.reduce((sum, r) => sum + r.score, 0) / results.length;
    console.log(`\n‚úÖ Phase ${phase} Average Score: ${avgScore.toFixed(2)}/5.0`);

    return {
      phase: phase,
      totalTests: results.length,
      averageScore: avgScore,
      results: results
    };
  }

  /**
   * Generate mock ADR output for testing
   */
  generateMockOutput(testCase, phase) {
    if (phase === "phase1") {
      return this.generatePhase1Output(testCase);
    } else if (phase === "phase2") {
      return this.generatePhase2Output(testCase);
    } else if (phase === "phase3") {
      return this.generatePhase3Output(testCase);
    }
    return "";
  }

  generatePhase1Output(testCase) {
    let decisionText = "";
    let positiveConsequences = "";
    let negativeConsequences = "";

    // Generate contextually appropriate decision and consequences
    if (testCase.id.includes("001")) {
      decisionText = "We will migrate from a monolithic architecture to domain-driven microservices, with each business domain owning its database and deploying independently. This addresses the 300% customer growth driving current deployment bottlenecks and team coordination overhead.";
      positiveConsequences = `- Reduces deployment time from 45 minutes to 5 minutes per service, eliminating synchronized release windows and production outage risks
- Enables individual teams to scale services independently based on actual demand; order processing can scale 10x without scaling inventory
- Allows domain teams to adopt appropriate technology stacks per domain; removes one-size-fits-all technology constraints
- Reduces team wait times on shared infrastructure; teams no longer block each other on database schema changes or deployment coordination`;
      negativeConsequences = `- Requires implementing distributed systems patterns: eventual consistency replaces ACID transactions, requiring event-driven architecture and compensation logic
- Increases network latency: inter-service calls add 50-100ms per hop, requiring caching strategies and circuit breakers throughout the system
- Requires new expertise in message queues (Kafka, RabbitMQ), distributed tracing (Jaeger), and service mesh; estimated 6-8 weeks team training
- Increases operational complexity: surface area grows from 1 deployment artifact to 10+, requiring Kubernetes orchestration and observability infrastructure`;
    } else if (testCase.id.includes("002")) {
      decisionText = "We will migrate our jQuery-based admin dashboard to React with TypeScript, establishing a component library shared across all frontend services. This addresses the 6-week onboarding time for new developers and reduces 40% of engineering time spent on bugs rather than features.";
      positiveConsequences = `- Reduces developer onboarding from 6 weeks to 2 weeks through component reusability and clear patterns
- Enables automated testing: component test coverage increases from 0% to 80%+ within first quarter through Jest and React Testing Library
- Reduces bug-related work from 40% to 10% of sprint time through TypeScript type safety preventing runtime errors and component isolation preventing state leaks
- Reduces initial page load from 8s to 2s through lazy-loading and bundle optimization; reduces time-to-interactive from 12s to 4s`;
      negativeConsequences = `- Requires React and TypeScript expertise; team needs 4-week ramp-up period before productive at target velocity
- Migration effort is 6-8 weeks for existing features; requires parallel implementation to avoid downtime
- Adds new toolchain burden with Webpack/Vite build system configuration, module bundling, and npm dependency management requirements
- Introduces npm ecosystem security and supply-chain risks; requires dependency scanning with Snyk/Dependabot and transitive dependency management training`;
    } else if (testCase.id.includes("003")) {
      decisionText = "We will migrate our relational database of 500GB primarily unstructured operational logs to a time-series database (InfluxDB or Prometheus), compressing and retaining only aggregated metrics beyond 30 days. This addresses $150k quarterly costs and 30-minute query latencies for analytics.";
      positiveConsequences = `- Reduces storage costs by 75% through compression and intelligent retention policies; quarterly cost drops from $150k to $37k
- Reduces analytics query latency from 30 minutes to 30 seconds through columnar storage and time-series optimization
- Enables real-time dashboards and automated alerts by supporting sub-second query response times for recent data windows (last 24 hours)
- Enforces automatic compliance with data deletion regulations (GDPR, CCPA) through granular, policy-based retention that requires no manual intervention`;
      negativeConsequences = `- Requires zero-downtime migration of 500GB data; dual-write period during migration increases database load 20%, requiring temporary capacity upgrade
- Requires analytics team retraining: time-series query semantics differ significantly from relational; 3-4 weeks learning curve for new query patterns and aggregation methods
- Eliminates relational query capabilities: complex joins become impossible; data models must restructure around time-series events rather than relational schemas
- Requires either managed service ($5-10k/month) or dedicated ops expertise: time-series database clustering, replication, and backup procedures differ from relational databases`;
    } else {
      decisionText = "We will establish centralized OAuth 2.0 authentication with single-sign-on (SSO), removing per-service login implementations. This addresses 12 security vulnerabilities from inconsistent auth and enables mandated multi-factor authentication across all services.";
      positiveConsequences = `- Reduces security vulnerabilities from 12 OWASP findings to 0 by eliminating session cookie handling bugs replicated across services
- Enables consistent password policy and MFA enforcement across all services; compliance audits now straightforward
- Simplifies user credential management: one password reset, one MFA enrollment enables access across all services
- Allows security team to perform one comprehensive review instead of auditing eight separate implementations`;
      negativeConsequences = `- Requires implementing single point of failure protection: authentication service outage blocks all dependent services; needs 99.99% SLA
- Increases authentication latency: every user request requires round-trip to auth service; requires aggressive token caching strategy
- Requires rewriting authentication flows in 8 different services; estimated 4-6 weeks engineering effort for coordinated rollout
- Requires new dependency on third-party identity provider (Okta, Auth0); adds $3-5k monthly cost and integration complexity`;
    }

    return `# ${testCase.name}

## Status
${testCase.status}

## Context
${testCase.context}

## Decision
${decisionText}

## Consequences

### Positive Consequences
${positiveConsequences}

### Negative Consequences
${negativeConsequences}`;
  }

  generatePhase2Output(testCase) {
    // Phase 2 produces an IMPROVED ADR, not just feedback
    // It refines the Phase 1 output based on quality assessment
    
    let improvedDecision = "";
    let improvedPositiveConsequences = "";
    let improvedNegativeConsequences = "";

    if (testCase.id.includes("001")) {
      improvedDecision = "We will migrate from a monolithic architecture to domain-driven microservices organized around business capabilities, with each domain owning its complete data store and deploying independently on a weekly cadence. This addresses the 300% customer growth that has made our current monolith unable to scale beyond deployments every 45 minutes, requiring coordinated releases across all services.";
      improvedPositiveConsequences = `- Reduces deployment time from 45-minute coordinated releases to 5-minute per-service deployments, eliminating production outage risk from cascading failures in single deployments
- Enables independent scaling: order-processing microservice handles peak traffic at 10x load without scaling inventory or shipping services, reducing infrastructure costs
- Allows domain teams to choose appropriate technology stacks and databases for their specific domain problems; removes monolithic technology constraints
- Reduces inter-team dependencies: teams deploy, scale, and debug independently without blocking on other teams' database schema changes or deployment windows`;
      improvedNegativeConsequences = `- Requires implementing event-driven architecture with message brokers (Kafka, RabbitMQ) for data consistency across domains; ACID transactions become impossible and teams must implement compensation logic instead
- Introduces significant network latency: inter-service calls add 50-100ms per hop; requires circuit breakers, bulkheads, and extensive caching strategies throughout the system
- Demands expertise in distributed systems, observability, and service meshes; current team requires 6-8 weeks of structured training on eventual consistency, distributed tracing (Jaeger/Zipkin), and Kubernetes
- Increases operational surface area from 1 deployment artifact to 10+ services; requires investment in containerization, service mesh infrastructure (Istio), and comprehensive monitoring across all services`;
    } else if (testCase.id.includes("002")) {
      improvedDecision = "We will migrate our jQuery-based admin dashboard to React with TypeScript, establishing a shared component library (Storybook) used across all four frontend services. This directly addresses the 6-week onboarding time for new frontend developers and the 40% of engineering time lost to frontend bugs rather than feature development.";
      improvedPositiveConsequences = `- Reduces developer onboarding from 6 weeks to 2 weeks by providing reusable, well-documented components and consistent patterns across all frontend applications
- Enables comprehensive automated testing: component test coverage increases from 0% to 80%+ in first quarter through Jest + React Testing Library; integration testing becomes feasible
- Reduces bug-related work from 40% to 10% of sprint velocity through TypeScript type safety preventing runtime errors and React component isolation preventing state leaks
- Improves frontend performance: bundle optimization and code-splitting reduce initial page load from 8s to 2s; lazy-loading reduces time-to-interactive from 12s to 4s`;
      improvedNegativeConsequences = `- Requires React and TypeScript expertise; team's current jQuery experts need 4-week intensive training before meeting target code velocity
- Migration effort spans 6-8 weeks of parallel development; requires maintaining both jQuery and React codebases simultaneously to avoid user-facing downtime
- Increases build infrastructure complexity with new toolchain (Webpack/Vite), module bundling, and npm dependency management; developers must understand tree-shaking and code-splitting concepts
- Introduces npm ecosystem security risks; team must implement dependency scanning (Snyk/Dependabot), manage transitive dependencies, and perform quarterly security audits`;
    } else if (testCase.id.includes("003")) {
      improvedDecision = "We will migrate our 500GB relational database containing primarily unstructured operational logs to a time-series database (InfluxDB or Prometheus), implementing rolling retention that keeps raw data for 30 days and only aggregated metrics beyond that. This solves the $150k quarterly storage cost and 30-minute analytics query latencies that block business decision-making.";
      improvedPositiveConsequences = `- Reduces storage costs from $150k quarterly to $37.5k quarterly through compression and intelligent retention policies, achieving 75% savings on database infrastructure
- Improves analytics query performance from 30 minutes to 30 seconds for typical reports through columnar storage and native time-series optimization
- Enables real-time operational dashboards and automated alerts by supporting sub-second queries on recent data (last 24 hours), allowing immediate detection of system anomalies
- Enforces automatic compliance with data deletion regulations (GDPR, CCPA) through granular, policy-based retention that requires no manual intervention`;
      improvedNegativeConsequences = `- Requires zero-downtime migration of 500GB of data; dual-write period during migration increases database load by 20%, requiring temporary capacity upgrade
- Requires analytics team retraining on time-series query semantics: traditional JOIN operations become impossible; data model must restructure around time-series events rather than relational schemas, estimated 3-4 weeks learning curve
- Eliminates relational capabilities: complex analytics queries requiring JOINs cannot be performed; historical analysis becomes difficult after 30-day retention window
- Increases operational complexity if self-hosted: time-series database clustering, replication, and backup procedures differ significantly from relational databases; requires managed service ($5-10k/month) or dedicated ops expertise`;
    } else {
      improvedDecision = "We will implement OAuth 2.0 single-sign-on (SSO) through a managed identity provider (Okta), consolidating authentication from eight separate service implementations into centralized access control. This resolves 12 OWASP authentication vulnerabilities from inconsistent implementations and enables mandatory multi-factor authentication across all services.";
      improvedPositiveConsequences = `- Eliminates 12 OWASP authentication vulnerabilities by consolidating session management, password handling, and credential validation into a single, audited service; reduces security review burden from eight separate implementations to one
- Enables organization-wide password policy, MFA enforcement, and conditional access rules; compliance audits become straightforward and repeatable
- Simplifies user experience: single password reset and single MFA enrollment grants access to all eight services; eliminates the cognitive load of managing eight separate credentials
- Reduces ongoing security maintenance burden: security team performs one quarterly review instead of auditing eight implementations; patches to auth logic apply globally`;
      improvedNegativeConsequences = `- Creates single point of failure: authentication service outage immediately blocks all dependent services; requires 99.99% uptime SLA and comprehensive disaster recovery plan with real-time failover
- Increases authentication latency: every user request requires round-trip to auth service (50-150ms added latency); requires aggressive token caching strategy and offline-capable UI patterns
- Requires rewriting authentication flows in eight services; estimated 4-6 weeks engineering effort for coordinated rollout with zero-downtime migration using token translation
- Introduces external dependency on managed identity provider: adds $3-5k monthly cost, vendor lock-in, and integration complexity; requires new OAuth implementation in all service clients`;
    }

    return `# ${testCase.name}

## Status
${testCase.status}

## Context
${testCase.context}

## Decision
${improvedDecision}

## Consequences

### Positive Consequences
${improvedPositiveConsequences}

### Negative Consequences
${improvedNegativeConsequences}`;
  }

  generatePhase3Output(testCase) {
    // Phase 3 synthesizes Phase 1 (draft) + Phase 2 (refined) into final ADR
    // Use the same high-quality outputs as Phase 2 for now, which are the refined versions
    let synthesizedDecision = "";
    let synthesizedPositiveConsequences = "";
    let synthesizedNegativeConsequences = "";

    if (testCase.id.includes("001")) {
      synthesizedDecision = "We will migrate from a monolithic architecture to domain-driven microservices organized around business capabilities, with each domain owning its complete data store and deploying independently on a weekly cadence. This directly addresses the 300% customer growth that has made our current monolith unable to scale beyond deployments every 45 minutes, requiring coordinated releases across all services that create production outage risk.";
      synthesizedPositiveConsequences = `- Reduces deployment time from 45-minute coordinated releases to 5-minute per-service deployments, eliminating cascading failure modes from synchronized releases and enabling weekly release cycles
- Enables independent scaling: individual services can handle 10x traffic without scaling the entire system, reducing infrastructure costs and improving resource utilization
- Allows domain teams to adopt technology stacks optimized for their specific problems rather than global constraints, enabling better long-term architectural flexibility
- Reduces inter-team dependencies and coordination overhead: teams can deploy, scale, and debug independently without blocking each other on database schema changes
- Improves time-to-market for features by removing queue time waiting for coordinated deployments and shared infrastructure`;
      synthesizedNegativeConsequences = `- Requires implementing distributed systems patterns: eventual consistency replaces ACID transactions, demanding event-driven architecture with compensating transactions instead of rollback safety
- Introduces significant network latency: inter-service calls add 50-100ms per hop; requires comprehensive circuit breakers, bulkheads, and caching strategies throughout the system
- Demands new expertise in distributed systems, service meshes, and observability; current team requires 6-8 weeks of structured training on eventual consistency, distributed tracing, and Kubernetes
- Increases operational complexity substantially: surface area grows from 1 deployment artifact to 10+ services requiring containerization, service mesh infrastructure (Istio), Kubernetes orchestration, and comprehensive monitoring
- Requires significant upfront investment in infrastructure and tooling before realizing benefits, with a 4-6 week migration period during which both systems must run in parallel`;
    } else if (testCase.id.includes("002")) {
      synthesizedDecision = "We will migrate our jQuery-based admin dashboard to React with TypeScript, establishing a shared component library (Storybook) used across all four frontend services. This directly addresses the 6-week onboarding time for new frontend developers and solves the 40% of engineering effort spent on frontend bugs rather than feature development.";
      synthesizedPositiveConsequences = `- Reduces developer onboarding from 6 weeks to 2 weeks by providing a well-documented component library, consistent patterns, and strong type safety across all frontend applications
- Enables comprehensive automated testing: component test coverage increases from 0% to 80%+ in the first quarter through Jest and React Testing Library, making integration testing feasible and reliable
- Reduces bug-related work from 40% to 10% of sprint velocity through TypeScript type safety preventing silent runtime errors and React component isolation preventing state leaks
- Improves frontend performance significantly: bundle optimization and code-splitting reduce initial page load from 8s to 2s; lazy-loading reduces time-to-interactive from 12s to 4s
- Creates a reusable asset that accelerates development of new frontend applications, multiplying the value of the initial investment`;
      synthesizedNegativeConsequences = `- Requires React and TypeScript expertise that current jQuery-focused team lacks; team needs 4-week intensive training before achieving target code velocity and productivity
- Migration effort spans 6-8 weeks of parallel development: requires maintaining both jQuery and React codebases simultaneously to avoid user-facing downtime or regressions
- Increases build infrastructure complexity significantly with new toolchain (Webpack/Vite), module bundling, tree-shaking, and npm dependency management
- Introduces npm ecosystem security and supply-chain risks; team must implement dependency scanning (Snyk/Dependabot) and maintain security practices for managing transitive dependencies
- Requires new DevOps capabilities: source map management, production debugging, and build optimization expertise that current team may not possess`;
    } else if (testCase.id.includes("003")) {
      synthesizedDecision = "We will migrate our 500GB relational database containing primarily unstructured operational logs to a time-series database (InfluxDB or Prometheus), implementing rolling retention that keeps raw data for 30 days and only aggregated metrics beyond that. This solves the $150k quarterly storage cost and 30-minute analytics query latencies that currently block business decision-making and real-time visibility.";
      synthesizedPositiveConsequences = `- Reduces storage costs from $150k quarterly to $37.5k quarterly through compression and intelligent retention policies, achieving 75% cost savings and improving quarterly margins
- Improves analytics query performance from 30 minutes to 30 seconds for typical reports through columnar storage and native time-series optimization, enabling business users to self-serve analytics
- Enables real-time operational dashboards and automated alerts by supporting sub-second queries on recent data (last 24 hours), allowing immediate detection and response to system anomalies
- Enforces automatic compliance with data deletion regulations (GDPR, CCPA) through granular, policy-based retention that requires no manual intervention or audit procedures
- Provides scalability path for growing data volume: time-series databases scale horizontally with data volume, unlike relational databases which hit hard limits`;
      synthesizedNegativeConsequences = `- Requires zero-downtime migration of 500GB of data: dual-write period during migration increases database load by 20%, requiring temporary capacity upgrade and careful coordination
- Requires analytics team retraining on time-series query semantics: traditional JOIN operations become impossible; data models must restructure around time-series events, estimated 3-4 weeks learning curve
- Eliminates relational query capabilities: complex analytics queries requiring JOINs cannot be performed; historical analysis becomes difficult after 30-day retention window for raw data
- Increases operational complexity if self-hosted: time-series database clustering, replication, and backup procedures differ significantly from relational databases; requires either $5-10k/month managed service or dedicated ops expertise
- Requires application code changes to emit data in time-series format rather than relational records, affecting all services that contribute to analytics`;
    } else {
      synthesizedDecision = "We will implement OAuth 2.0 single-sign-on (SSO) through a managed identity provider (Okta), consolidating authentication from eight separate service implementations into centralized, audited access control. This resolves 12 OWASP authentication vulnerabilities from inconsistent implementations and enables mandatory multi-factor authentication across all services.";
      synthesizedPositiveConsequences = `- Eliminates 12 OWASP authentication vulnerabilities by consolidating session management, password handling, and credential validation into a single, professionally audited service; reduces security review burden from eight separate implementations
- Enables organization-wide password policy enforcement, MFA mandatory requirement, and conditional access rules; compliance audits become straightforward, repeatable, and can be run in minutes instead of weeks
- Simplifies user credential management: single password reset and single MFA enrollment grants access to all eight services; eliminates cognitive load and support tickets from managing multiple credentials
- Reduces ongoing security maintenance burden: security team performs one quarterly review instead of auditing eight implementations separately; patches and security updates apply globally
- Provides audit trail capability: centralized logging of all authentication events enables security monitoring, suspicious activity detection, and regulatory compliance reporting`;
      synthesizedNegativeConsequences = `- Creates single point of failure: authentication service outage immediately blocks all dependent services globally; requires 99.99% uptime SLA, comprehensive disaster recovery, and real-time failover mechanisms
- Increases authentication latency: every user request requires round-trip to auth service (50-150ms added latency per request); requires aggressive token caching strategy, offline-capable UI patterns, and refresh token logic
- Requires rewriting authentication flows in eight different services: estimated 4-6 weeks engineering effort for coordinated rollout with zero-downtime migration using token translation and dual authentication
- Introduces external dependency on managed identity provider: adds $3-5k monthly recurring cost, vendor lock-in, and integration complexity; requires new OAuth implementation knowledge across all service teams
- Adds network dependency and reliability concern: if Okta experiences latency or outage, all services are affected; requires careful consideration of fallback and degradation strategies`;
    }

    return `# ${testCase.name}

## Status
${testCase.status}

## Context
${testCase.context}

## Decision
${synthesizedDecision}

## Consequences

### Positive Consequences
${synthesizedPositiveConsequences}

### Negative Consequences
${synthesizedNegativeConsequences}`;
  }

  /**
   * Save test results to disk
   */
  saveResults(phase, results) {
    if (!fs.existsSync(this.resultsDir)) {
      fs.mkdirSync(this.resultsDir, { recursive: true });
    }

    const timestamp = new Date().toISOString();
    const filename = `${phase}_results_${timestamp.split("T")[0]}.json`;
    const filepath = path.join(this.resultsDir, filename);

    fs.writeFileSync(filepath, JSON.stringify(results, null, 2));
    console.log(`\nüíæ Results saved to: ${filepath}`);
  }

  /**
   * Analyze all test results and generate improvement suggestions
   */
  analyzeResults(phase) {
    console.log(`\nüìä Analyzing ${phase} Results`);
    console.log("=" .repeat(60));

    // In a real implementation, this would read actual test results
    // For now, we'll show the analysis structure

    const suggestions = {
      phase1: [
        {
          area: "Decision Clarity",
          current: "Decisions use vague language like \"strategic approach\" and \"critical intervention\"",
          suggestion: "Be specific: name the architectural pattern (e.g., \"microservices\", \"monorepo\", \"event-driven\")",
          priority: "HIGH",
          impact: "Will improve clarity score from 3.2 to 4.5"
        },
        {
          area: "Consequences Balance",
          current: "Negative consequences are still somewhat generic",
          suggestion: "Replace \"complexity\" with specific impacts: \"requires event-driven patterns\", \"adds network latency\", \"needs distributed tracing\"",
          priority: "HIGH",
          impact: "Will improve balance score from 3.8 to 4.7"
        },
        {
          area: "Context Grounding",
          current: "Decision sometimes feels disconnected from specific context provided",
          suggestion: "Reference specific numbers from context: \"300% growth\", \"2-3 hour outages\", \"45-minute deployments\"",
          priority: "MEDIUM",
          impact: "Will improve soundness score from 3.5 to 4.2"
        }
      ],
      phase2: [
        {
          area: "Review Depth",
          current: "Reviews are somewhat surface-level",
          suggestion: "Go deeper into architectural tradeoffs: ask about alternative approaches, scalability limits, team skills gaps",
          priority: "HIGH",
          impact: "Will make phase synthesis more effective"
        },
        {
          area: "Constructive Specificity",
          current: "Suggestions are sometimes generic",
          suggestion: "Provide concrete examples: instead of \"more specific\", say \"name the service boundary pattern\"",
          priority: "MEDIUM",
          impact: "Phase 3 will have clearer direction"
        }
      ],
      phase3: [
        {
          area: "Synthesis Quality",
          current: "Synthesis sometimes picks middle ground instead of best arguments",
          suggestion: "Actively choose Phase 1 OR Phase 2 language where it's superior, don't average",
          priority: "HIGH",
          impact: "Will boost final score from 3.9 to 4.6"
        },
        {
          area: "Rationale Clarity",
          current: "Final ADR doesn't always explain why this decision over alternatives",
          suggestion: "Add one sentence in Decision explaining the primary rationale",
          priority: "MEDIUM",
          impact: "Will improve technical soundness score"
        }
      ]
    };

    return suggestions[phase] || [];
  }

  /**
   * Get improvement suggestions for a phase
   */
  suggestImprovements(phase) {
    console.log(`\nüí° Improvement Suggestions for ${phase.toUpperCase()}`);
    console.log("=" .repeat(60));

    const suggestions = this.analyzeResults(phase);

    for (const suggestion of suggestions) {
      console.log(`\nüìå ${suggestion.area}`);
      console.log(`   Priority: ${suggestion.priority}`);
      console.log(`   Current: ${suggestion.current}`);
      console.log(`   ‚Üí Suggestion: ${suggestion.suggestion}`);
      console.log(`   Impact: ${suggestion.impact}`);
    }

    return suggestions;
  }
}

// CLI interface
const args = process.argv.slice(2);
const command = args[0];
const phase = args[1] || "phase1";
const mockMode = args.includes("--mock");

const tuner = new ADRPromptTuner();

(async() => {
  try {
    if (command === "test") {
      await tuner.testPhase(phase, mockMode);
    } else if (command === "analyze-results") {
      tuner.analyzeResults(phase);
    } else if (command === "suggest-improvements") {
      tuner.suggestImprovements(phase);
    } else {
      console.log(`
ADR Prompt Tuning Tool
Usage:
  node tools/prompt-tuner.js test <phase> [--mock]       Test a prompt phase
  node tools/prompt-tuner.js analyze-results <phase>     Analyze test results
  node tools/prompt-tuner.js suggest-improvements <phase> Get improvement suggestions

Phases: phase1, phase2, phase3

Example:
  node tools/prompt-tuner.js test phase1 --mock
  node tools/prompt-tuner.js suggest-improvements phase1
      `);
    }
  } catch (error) {
    console.error(`‚ùå Error: ${error.message}`);
    process.exit(1);
  }
})();

export default ADRPromptTuner;
